{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T21:10:02.590660Z",
     "start_time": "2021-11-24T21:09:51.612958Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "\n",
    "import spacy\n",
    "nlp=spacy.load('es_core_news_sm')\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "spanish_stemmer = SnowballStemmer('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T21:10:10.382587Z",
     "start_time": "2021-11-24T21:10:10.370555Z"
    }
   },
   "outputs": [],
   "source": [
    "respuestas = pd.read_csv('tbl_respuestas.csv', sep=',', encoding='utf_8')\n",
    "respuestas.drop(\"Modalidad\", axis=1, inplace=True)\n",
    "#respuestas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T21:10:14.088543Z",
     "start_time": "2021-11-24T21:10:14.045468Z"
    }
   },
   "outputs": [],
   "source": [
    "#funciones\n",
    "#####################################################################################################\n",
    "def PreProcesar_carrera(Corpus, POS=False, Lema=True, Stem=True):\n",
    "    \n",
    "    \n",
    "    # Generar una lista de documentos de spacy para tratar el POS Tagging y la Lematización\n",
    "    docs=[]\n",
    "    for oracion in Corpus:\n",
    "        docs.append(nlp(oracion.lower())) #La lematización funciona mejor en minúsculas\n",
    "    \n",
    "    # Crear una lista de oraciones, donde cada elemento es una lista de palabras.\n",
    "    # Cada palabra está definida por una tupla (Texto, POSTag, Lema)\n",
    "    # Se omiten los tokens que son identificados como signos de puntuación\n",
    "    oraciones=[]\n",
    "    for doc in docs:\n",
    "        oracion=[]\n",
    "        for token in doc:\n",
    "            if token.pos_ != 'PUNCT':\n",
    "                oracion.append((token.text, token.pos_, token.lemma_))\n",
    "        oraciones.append(oracion)\n",
    "    \n",
    "    fc = open('stopwords.txt', 'r', encoding='utf8')\n",
    "    stopwords = fc.read().split('\\n')\n",
    "    fc.close()\n",
    "    oraciones = [[palabra for palabra in oracion if palabra[2] not in stopwords] for oracion in oraciones]\n",
    "    \n",
    "    # Stemming\n",
    "    if Stem==True:\n",
    "        oraciones_aux=[]\n",
    "        for oracion in oraciones:\n",
    "            oracion_aux=[]\n",
    "            for palabra in oracion:\n",
    "                p_texto, p_pos, p_lema = palabra\n",
    "                # Si Lema es True, se Stemmatiza el lema; si no, se Stemmatiza la palabra original\n",
    "                if Lema==True:\n",
    "                    oracion_aux.append((p_texto, p_pos, p_lema, spanish_stemmer.stem(p_lema)))\n",
    "                else:\n",
    "                    oracion_aux.append((p_texto, p_pos, p_lema, spanish_stemmer.stem(p_texto)))\n",
    "            oraciones_aux.append(oracion_aux)\n",
    "        \n",
    "        oraciones = oraciones_aux\n",
    "    \n",
    "    Corpus_Procesado = [] #Variable de salida\n",
    "    \n",
    "    for doc in oraciones:\n",
    "        oracion = ''\n",
    "        for palabra in doc:\n",
    "            if Stem == True:\n",
    "                # Devolver cadena de Stemming\n",
    "                oracion = oracion + palabra[3]\n",
    "            else:\n",
    "                if Lema == True:\n",
    "                    # Devolver cadena de Lemas\n",
    "                    oracion = oracion + palabra[2]\n",
    "                else:\n",
    "                    # Devolver cadena de palabras originales\n",
    "                    oracion = oracion + palabra[0]\n",
    "            \n",
    "            if POS == True:\n",
    "                #Concatenar POS a cada palabra\n",
    "                oracion = oracion + '_' + palabra[1].lower()\n",
    "            \n",
    "            oracion = oracion + ' '\n",
    "        \n",
    "        Corpus_Procesado.append(oracion)\n",
    "        \n",
    "    return Corpus_Procesado\n",
    "\n",
    "def Corregir_Documentos_carrera(df_textos, columnas, POS=False, Lema=True, Stem=True):\n",
    "\n",
    "    for col in columnas:\n",
    "        df_textos[col] = PreProcesar_carrera(list(df_textos[col]), POS, Lema, Stem)\n",
    "    \n",
    "    # Sanear el DataFrame eliminando los duplicados y reindexándolo\n",
    "    df_textos = df_textos.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return df_textos\n",
    "\n",
    "def carrera_lista(df_textos):\n",
    "    vari_df_textos_carr= df_textos.copy()\n",
    "    carrera_corr = Corregir_Documentos_carrera(vari_df_textos_carr,['oracion'],False,True,True)\n",
    "\n",
    "    vectorizador_carreras = pickle.load(open(\"vectorizador_carreras.pkl\",\"rb\"))\n",
    "    array_carreras=vectorizador_carreras.transform([carrera_corr['oracion'][0][:-1]])\n",
    "\n",
    "    modelo_carreras = pickle.load(open(\"modelo_carreras.sav\",\"rb\"))\n",
    "\n",
    "    carrera =sorted(list(modelo_carreras.predict_proba(array_carreras)[0]))[-1]\n",
    "    carrera2 = modelo_carreras.predict(array_carreras)\n",
    "    if carrera < 0.7: \n",
    "        carrera2 = [\"todas\"]\n",
    "    return carrera2,carrera\n",
    "#####################################################################################################\n",
    "def PreProcesar_w5(Corpus, POS=False, Lema=True, Stem=True):\n",
    "    \n",
    "    \n",
    "    # Generar una lista de documentos de spacy para tratar el POS Tagging y la Lematización\n",
    "    docs=[]\n",
    "    for oracion in Corpus:\n",
    "        docs.append(nlp(oracion.lower())) #La lematización funciona mejor en minúsculas\n",
    "    \n",
    "    # Crear una lista de oraciones, donde cada elemento es una lista de palabras.\n",
    "    # Cada palabra está definida por una tupla (Texto, POSTag, Lema)\n",
    "    # Se omiten los tokens que son identificados como signos de puntuación\n",
    "    oraciones=[]\n",
    "    for doc in docs:\n",
    "        oracion=[]\n",
    "        for token in doc:\n",
    "            if token.pos_ != 'PUNCT':\n",
    "                oracion.append((token.text, token.pos_, token.lemma_))\n",
    "        oraciones.append(oracion)\n",
    "    \n",
    "    ww = open('stopwords_sin_w5.txt', 'r', encoding='utf8')\n",
    "    stopwords = ww.read().split('\\n')\n",
    "    #stopwords=[x.lower() for x in stopwords]\n",
    "    ww.close()\n",
    "    oraciones = [[palabra for palabra in oracion if palabra[2] not in stopwords] for oracion in oraciones]\n",
    "    \n",
    "    # Stemming\n",
    "    if Stem==True:\n",
    "        oraciones_aux=[]\n",
    "        for oracion in oraciones:\n",
    "            oracion_aux=[]\n",
    "            for palabra in oracion:\n",
    "                p_texto, p_pos, p_lema = palabra\n",
    "                # Si Lema es True, se Stemmatiza el lema; si no, se Stemmatiza la palabra original\n",
    "                if Lema==True:\n",
    "                    oracion_aux.append((p_texto, p_pos, p_lema, spanish_stemmer.stem(p_lema)))\n",
    "                else:\n",
    "                    oracion_aux.append((p_texto, p_pos, p_lema, spanish_stemmer.stem(p_texto)))\n",
    "            oraciones_aux.append(oracion_aux)\n",
    "        \n",
    "        oraciones = oraciones_aux\n",
    "    \n",
    "    Corpus_Procesado = [] #Variable de salida\n",
    "    \n",
    "    for doc in oraciones:\n",
    "        oracion = ''\n",
    "        for palabra in doc:\n",
    "            if Stem == True:\n",
    "                # Devolver cadena de Stemming\n",
    "                oracion = oracion + palabra[3]\n",
    "            else:\n",
    "                if Lema == True:\n",
    "                    # Devolver cadena de Lemas\n",
    "                    oracion = oracion + palabra[2]\n",
    "                else:\n",
    "                    # Devolver cadena de palabras originales\n",
    "                    oracion = oracion + palabra[0]\n",
    "            \n",
    "            if POS == True:\n",
    "                #Concatenar POS a cada palabra\n",
    "                oracion = oracion + '_' + palabra[1].lower()\n",
    "            \n",
    "            oracion = oracion + ' '\n",
    "        \n",
    "        Corpus_Procesado.append(oracion)\n",
    "        \n",
    "    return Corpus_Procesado\n",
    "\n",
    "def Corregir_Documentos_w5(df_textos, columnas, POS=False, Lema=True, Stem=True):\n",
    "\n",
    "    for col in columnas:\n",
    "        df_textos[col] = PreProcesar_w5(list(df_textos[col]), POS, Lema, Stem)\n",
    "    \n",
    "    # Sanear el DataFrame eliminando los duplicados y reindexándolo\n",
    "    df_textos = df_textos.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return df_textos\n",
    "\n",
    "def w5_listo(df_textos):\n",
    "    vari_df_textos_w5= df_textos.copy()\n",
    "    w5_corr = Corregir_Documentos_w5(vari_df_textos_w5,['oracion'],False,True,True)\n",
    "    vectorizador_w5 = pickle.load(open(\"vectorizador_w5.pkl\",\"rb\"))\n",
    "    array_w5=vectorizador_w5.transform([w5_corr['oracion'][0][:-1]])\n",
    "    modelo_w5 = pickle.load(open(\"modelo_w5.sav\",\"rb\"))\n",
    "\n",
    "    w5 = sorted(list(modelo_w5.predict_proba(array_w5)[0]))[-1]\n",
    "    w5_2 = modelo_w5.predict(array_w5)\n",
    "    if w5 < 0.7: w5_2[0] = \"todas\"\n",
    "    return w5_2,w5\n",
    "#####################################################################################################\n",
    "def PreProcesar_intents(Corpus, POS=False, Lema=True, Stem=True):\n",
    "    \n",
    "    \n",
    "    # Generar una lista de documentos de spacy para tratar el POS Tagging y la Lematización\n",
    "    docs=[]\n",
    "    for oracion in Corpus:\n",
    "        docs.append(nlp(oracion.lower())) #La lematización funciona mejor en minúsculas\n",
    "    \n",
    "    # Crear una lista de oraciones, donde cada elemento es una lista de palabras.\n",
    "    # Cada palabra está definida por una tupla (Texto, POSTag, Lema)\n",
    "    # Se omiten los tokens que son identificados como signos de puntuación\n",
    "    oraciones=[]\n",
    "    for doc in docs:\n",
    "        oracion=[]\n",
    "        for token in doc:\n",
    "            if token.pos_ != 'PUNCT':\n",
    "                oracion.append((token.text, token.pos_, token.lemma_))\n",
    "        oraciones.append(oracion)\n",
    "    \n",
    "    ww = open('stopwords_intents.txt', 'r', encoding='utf8')\n",
    "    stopwords = ww.read().split('\\n')\n",
    "    stopwords=[x.lower() for x in stopwords]\n",
    "    ww.close()\n",
    "    oraciones = [[palabra for palabra in oracion if palabra[2] not in stopwords] for oracion in oraciones]\n",
    "    \n",
    "    # Stemming\n",
    "    if Stem==True:\n",
    "        oraciones_aux=[]\n",
    "        for oracion in oraciones:\n",
    "            oracion_aux=[]\n",
    "            for palabra in oracion:\n",
    "                p_texto, p_pos, p_lema = palabra\n",
    "                # Si Lema es True, se Stemmatiza el lema; si no, se Stemmatiza la palabra original\n",
    "                if Lema==True:\n",
    "                    oracion_aux.append((p_texto, p_pos, p_lema, spanish_stemmer.stem(p_lema)))\n",
    "                else:\n",
    "                    oracion_aux.append((p_texto, p_pos, p_lema, spanish_stemmer.stem(p_texto)))\n",
    "            oraciones_aux.append(oracion_aux)\n",
    "        \n",
    "        oraciones = oraciones_aux\n",
    "    \n",
    "    Corpus_Procesado = [] #Variable de salida\n",
    "    \n",
    "    for doc in oraciones:\n",
    "        oracion = ''\n",
    "        for palabra in doc:\n",
    "            if Stem == True:\n",
    "                # Devolver cadena de Stemming\n",
    "                oracion = oracion + palabra[3]\n",
    "            else:\n",
    "                if Lema == True:\n",
    "                    # Devolver cadena de Lemas\n",
    "                    oracion = oracion + palabra[2]\n",
    "                else:\n",
    "                    # Devolver cadena de palabras originales\n",
    "                    oracion = oracion + palabra[0]\n",
    "            \n",
    "            if POS == True:\n",
    "                #Concatenar POS a cada palabra\n",
    "                oracion = oracion + '_' + palabra[1].lower()\n",
    "            \n",
    "            oracion = oracion + ' '\n",
    "        \n",
    "        Corpus_Procesado.append(oracion)\n",
    "        \n",
    "    return Corpus_Procesado\n",
    "\n",
    "def Corregir_Documentos_intents(df_textos, columnas, POS=False, Lema=True, Stem=True):\n",
    "\n",
    "    for col in columnas:\n",
    "        df_textos[col] = PreProcesar_intents(list(df_textos[col]), POS, Lema, Stem)\n",
    "    \n",
    "    # Sanear el DataFrame eliminando los duplicados y reindexándolo\n",
    "    df_textos = df_textos.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return df_textos\n",
    "\n",
    "def inte_listo(df_textos):\n",
    "    vari_df_textos_inte = df_textos.copy()\n",
    "    inte_corr = Corregir_Documentos_intents(vari_df_textos_inte,['oracion'],False,True,True)\n",
    "    vectorizador_intents = pickle.load(open(\"vectorizador_intents.pkl\",\"rb\"))\n",
    "    array_intents=vectorizador_intents.transform([inte_corr['oracion'][0][:-1]])\n",
    "\n",
    "    modelo_intents = pickle.load(open(\"modelo_intents.sav\",\"rb\"))\n",
    "    intents = sorted(list(modelo_intents.predict_proba(array_intents)[0]))[-1]\n",
    "    intents_2 = modelo_intents.predict(array_intents)\n",
    "    if intents < 0.7: intents_2[0] = \"generalidades\"\n",
    "    return intents_2,intents\n",
    "#####################################################################################################\n",
    "def sub_inte_listo(df_textos):\n",
    "    vari_df_textos_sub_inte= df_textos.copy()\n",
    "    sub_inte_corr = Corregir_Documentos_intents(vari_df_textos_sub_inte,['oracion'],False,True,True)\n",
    "    vectorizador_sub_intents = pickle.load(open(\"vectorizador_sub_intents.pkl\",\"rb\"))\n",
    "    array_sub_intents=vectorizador_sub_intents.transform([sub_inte_corr['oracion'][0][:-1]])\n",
    "\n",
    "    modelo_sub_intents = pickle.load(open(\"modelo_sub_intents.sav\",\"rb\"))\n",
    "    sub_intents = sorted(list(modelo_sub_intents.predict_proba(array_sub_intents)[0]))[-1]\n",
    "    sub_intents_2 = modelo_sub_intents.predict(array_sub_intents)\n",
    "    if sub_intents < 0.7: sub_intents_2[0] = \"todas\"\n",
    "    return sub_intents_2,sub_intents\n",
    "#####################################################################################################\n",
    "def pregunta_lista_(df_textos):\n",
    "    carrera_ = carrera_lista(df_textos)\n",
    "    w5_ = w5_listo(df_textos)\n",
    "    inte_ = inte_listo(df_textos)\n",
    "    sub_inte_ = sub_inte_listo(df_textos)\n",
    "    listita = [inte_[0][0], sub_inte_[0][0], carrera_[0][0], w5_[0][0]]\n",
    "    df_textos_listo = pd.DataFrame(columns = ['Intencion',\"SubIntencion\",\"Carrera\",\"w5\"])\n",
    "    df_textos_listo.loc[0] = listita\n",
    "    return df_textos_listo \n",
    "#####################################################################################################\n",
    "def respuesta(df_textos):\n",
    "    pregunta_lista = pregunta_lista_(df_textos)\n",
    "    respu = respuestas[(respuestas.Intencion==pregunta_lista[\"Intencion\"].values[0])\\\n",
    "                & (respuestas.SubIntencion==pregunta_lista[\"SubIntencion\"].values[0])\\\n",
    "                & (respuestas.Carrera==pregunta_lista[\"Carrera\"].values[0])\\\n",
    "                & ((respuestas.w5==pregunta_lista[\"w5\"].values[0]) | (respuestas.w5==\"todas\"))] \n",
    "    if len(respu.index) < 1: print(\"Perdon, no entendi bien la pregunta. ¿Podrias reformularla? :D \")                \n",
    "    else: print(str(respu[\"Respuesta\"].values[0]))\n",
    "    #return respu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-24T21:10:58.009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola, en qué te puedo ayudar?\n",
      "\n",
      "texto: 3d\n",
      "tu:  3d\n",
      "La revolución tecnológica presenta escenarios continuamente cambiantes en nuestra vida y en la de las organizaciones. En este contexto, la Impresión 3D entra en nuestros hogares a través de juguetes, indumentaria, herramientas, muebles, prótesis, y especialmente en la medicina, la odontología y la industria, alcanzando todos los ámbitos. De este modo, la impresión 3D presenta un nuevo horizonte que se adapta a las necesidades de los mercados, donde el profesional en Diseño e Impresión 3D está capacitado para planificar y programar actividades relacionadas con el diseño y la fabricación de productos, integrar equipos técnicos asociados a la producción; ofrecer diseños, prototipos o aplicaciones finales que se puedan materializar en poco tiempo y a bajo costo.\n",
      "bot:  None\n",
      "Tiene otra pregunta? s/n\n",
      "s\n",
      "Hola, en qué te puedo ayudar?\n",
      "\n",
      "texto: administracion\n",
      "tu:  administracion\n",
      "Perdon, no entendi bien la pregunta. ¿Podrias reformularla? :D \n",
      "bot:  None\n",
      "Tiene otra pregunta? s/n\n"
     ]
    }
   ],
   "source": [
    "on = True\n",
    "while on:\n",
    "    print(\"Hola, en qué te puedo ayudar?\\n\")\n",
    "    pregunta = input(\"texto: \")\n",
    "    print(\"tu: \",pregunta)\n",
    "    df_textos = pd.DataFrame(columns = ['oracion'])\n",
    "    df_textos.loc[0] = [pregunta] \n",
    "    df_textos \n",
    "    carrera_lista(df_textos)\n",
    "    w5_listo(df_textos)\n",
    "    inte_listo(df_textos)\n",
    "    sub_inte_listo(df_textos)\n",
    "    pregunta_lista_(df_textos)\n",
    "    print(\"bot: \",respuesta(df_textos))\n",
    "    print(\"Tiene otra pregunta? s/n\")\n",
    "    rta=input()\n",
    "    if rta==\"s\":\n",
    "        on=True\n",
    "    elif rta==\"n\":\n",
    "        on=False\n",
    "    elif (rta != \"s\" ) | (rta != \"n\"):\n",
    "        print(\"Opcion incorrecta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d333c3e79956f6cfdda154d497169890c9e1b3b648807dd58683480f0849f8e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
