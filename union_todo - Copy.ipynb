{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T21:10:02.590660Z",
     "start_time": "2021-11-24T21:09:51.612958Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "\n",
    "import spacy\n",
    "nlp=spacy.load('es_core_news_sm')\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "spanish_stemmer = SnowballStemmer('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T21:10:10.382587Z",
     "start_time": "2021-11-24T21:10:10.370555Z"
    }
   },
   "outputs": [],
   "source": [
    "respuestas = pd.read_csv('tbl_respuestas.csv', sep=',', encoding='utf_8')\n",
    "respuestas.drop(\"Modalidad\", axis=1, inplace=True)\n",
    "#respuestas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T21:10:14.088543Z",
     "start_time": "2021-11-24T21:10:14.045468Z"
    }
   },
   "outputs": [],
   "source": [
    "#funciones\n",
    "#####################################################################################################\n",
    "def PreProcesar_carrera(Corpus, POS=False, Lema=True, Stem=True):\n",
    "    \n",
    "    \n",
    "    # Generar una lista de documentos de spacy para tratar el POS Tagging y la Lematización\n",
    "    docs=[]\n",
    "    for oracion in Corpus:\n",
    "        docs.append(nlp(oracion.lower())) #La lematización funciona mejor en minúsculas\n",
    "    \n",
    "    # Crear una lista de oraciones, donde cada elemento es una lista de palabras.\n",
    "    # Cada palabra está definida por una tupla (Texto, POSTag, Lema)\n",
    "    # Se omiten los tokens que son identificados como signos de puntuación\n",
    "    oraciones=[]\n",
    "    for doc in docs:\n",
    "        oracion=[]\n",
    "        for token in doc:\n",
    "            if token.pos_ != 'PUNCT':\n",
    "                oracion.append((token.text, token.pos_, token.lemma_))\n",
    "        oraciones.append(oracion)\n",
    "    \n",
    "    \"\"\"fc = open('stopwords.txt', 'r', encoding='utf8')\n",
    "    stopwords = fc.read().split('\\n')\n",
    "    fc.close()\n",
    "    oraciones = [[palabra for palabra in oracion if palabra[2] not in stopwords] for oracion in oraciones]\"\"\"\n",
    "    \n",
    "    # Stemming\n",
    "    if Stem==True:\n",
    "        oraciones_aux=[]\n",
    "        for oracion in oraciones:\n",
    "            oracion_aux=[]\n",
    "            for palabra in oracion:\n",
    "                p_texto, p_pos, p_lema = palabra\n",
    "                # Si Lema es True, se Stemmatiza el lema; si no, se Stemmatiza la palabra original\n",
    "                if Lema==True:\n",
    "                    oracion_aux.append((p_texto, p_pos, p_lema, spanish_stemmer.stem(p_lema)))\n",
    "                else:\n",
    "                    oracion_aux.append((p_texto, p_pos, p_lema, spanish_stemmer.stem(p_texto)))\n",
    "            oraciones_aux.append(oracion_aux)\n",
    "        \n",
    "        oraciones = oraciones_aux\n",
    "    \n",
    "    Corpus_Procesado = [] #Variable de salida\n",
    "    \n",
    "    for doc in oraciones:\n",
    "        oracion = ''\n",
    "        for palabra in doc:\n",
    "            if Stem == True:\n",
    "                # Devolver cadena de Stemming\n",
    "                oracion = oracion + palabra[3]\n",
    "            else:\n",
    "                if Lema == True:\n",
    "                    # Devolver cadena de Lemas\n",
    "                    oracion = oracion + palabra[2]\n",
    "                else:\n",
    "                    # Devolver cadena de palabras originales\n",
    "                    oracion = oracion + palabra[0]\n",
    "            \n",
    "            if POS == True:\n",
    "                #Concatenar POS a cada palabra\n",
    "                oracion = oracion + '_' + palabra[1].lower()\n",
    "            \n",
    "            oracion = oracion + ' '\n",
    "        \n",
    "        Corpus_Procesado.append(oracion)\n",
    "        \n",
    "    return Corpus_Procesado\n",
    "\n",
    "def Corregir_Documentos_carrera(df_textos, columnas, POS=False, Lema=True, Stem=True):\n",
    "\n",
    "    for col in columnas:\n",
    "        df_textos[col] = PreProcesar_carrera(list(df_textos[col]), POS, Lema, Stem)\n",
    "    \n",
    "    # Sanear el DataFrame eliminando los duplicados y reindexándolo\n",
    "    df_textos = df_textos.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return df_textos\n",
    "\n",
    "def carrera_lista(df_textos):\n",
    "    \n",
    "    vari_df_textos_carr= df_textos.copy()\n",
    "    print(\"vari_df_textos_carr\",vari_df_textos_carr)#borrar\n",
    "    carrera_corr = Corregir_Documentos_carrera(vari_df_textos_carr,['oracion'],False,True,True)\n",
    "    print(\"carrera_lista carrera_corr\")#borrar\n",
    "    print(carrera_corr)#borrar\n",
    "    vectorizador_carreras = pickle.load(open(\"vectorizador_carreras.pkl\",\"rb\"))\n",
    "    print(vectorizador_carreras.get_feature_names()) #borrar\n",
    "    array_carreras=vectorizador_carreras.transform([carrera_corr['oracion'][0][:-1]])\n",
    "    for i in range(len(array_carreras.toarray()[0])):#borrar\n",
    "        if array_carreras.toarray()[0][i]>0:#borrar\n",
    "            print(array_carreras.toarray()[0][i],vectorizador_carreras.get_feature_names()[i]) #borrar\n",
    "\n",
    "    modelo_carreras = pickle.load(open(\"modelo_carreras.sav\",\"rb\"))\n",
    "\n",
    "    carrera =sorted(list(modelo_carreras.predict_proba(array_carreras)[0]))[-1]\n",
    "    carrera2 = modelo_carreras.predict(array_carreras)\n",
    "    if carrera < 0.7: \n",
    "        carrera2 = [\"todas\"]\n",
    "    return carrera2,carrera\n",
    "#####################################################################################################\n",
    "def PreProcesar_w5(Corpus, POS=False, Lema=True, Stem=True):\n",
    "    \n",
    "    \n",
    "    # Generar una lista de documentos de spacy para tratar el POS Tagging y la Lematización\n",
    "    docs=[]\n",
    "    for oracion in Corpus:\n",
    "        docs.append(nlp(oracion.lower())) #La lematización funciona mejor en minúsculas\n",
    "    \n",
    "    # Crear una lista de oraciones, donde cada elemento es una lista de palabras.\n",
    "    # Cada palabra está definida por una tupla (Texto, POSTag, Lema)\n",
    "    # Se omiten los tokens que son identificados como signos de puntuación\n",
    "    oraciones=[]\n",
    "    for doc in docs:\n",
    "        oracion=[]\n",
    "        for token in doc:\n",
    "            if token.pos_ != 'PUNCT':\n",
    "                oracion.append((token.text, token.pos_, token.lemma_))\n",
    "        oraciones.append(oracion)\n",
    "    \n",
    "    ww = open('stopwords_sin_w5.txt', 'r', encoding='utf8')\n",
    "    stopwords = ww.read().split('\\n')\n",
    "    #stopwords=[x.lower() for x in stopwords]\n",
    "    ww.close()\n",
    "    oraciones = [[palabra for palabra in oracion if palabra[2] not in stopwords] for oracion in oraciones]\n",
    "    \n",
    "    # Stemming\n",
    "    if Stem==True:\n",
    "        oraciones_aux=[]\n",
    "        for oracion in oraciones:\n",
    "            oracion_aux=[]\n",
    "            for palabra in oracion:\n",
    "                p_texto, p_pos, p_lema = palabra\n",
    "                # Si Lema es True, se Stemmatiza el lema; si no, se Stemmatiza la palabra original\n",
    "                if Lema==True:\n",
    "                    oracion_aux.append((p_texto, p_pos, p_lema, spanish_stemmer.stem(p_lema)))\n",
    "                else:\n",
    "                    oracion_aux.append((p_texto, p_pos, p_lema, spanish_stemmer.stem(p_texto)))\n",
    "            oraciones_aux.append(oracion_aux)\n",
    "        \n",
    "        oraciones = oraciones_aux\n",
    "    \n",
    "    Corpus_Procesado = [] #Variable de salida\n",
    "    \n",
    "    for doc in oraciones:\n",
    "        oracion = ''\n",
    "        for palabra in doc:\n",
    "            if Stem == True:\n",
    "                # Devolver cadena de Stemming\n",
    "                oracion = oracion + palabra[3]\n",
    "            else:\n",
    "                if Lema == True:\n",
    "                    # Devolver cadena de Lemas\n",
    "                    oracion = oracion + palabra[2]\n",
    "                else:\n",
    "                    # Devolver cadena de palabras originales\n",
    "                    oracion = oracion + palabra[0]\n",
    "            \n",
    "            if POS == True:\n",
    "                #Concatenar POS a cada palabra\n",
    "                oracion = oracion + '_' + palabra[1].lower()\n",
    "            \n",
    "            oracion = oracion + ' '\n",
    "        \n",
    "        Corpus_Procesado.append(oracion)\n",
    "        \n",
    "    return Corpus_Procesado\n",
    "\n",
    "def Corregir_Documentos_w5(df_textos, columnas, POS=False, Lema=True, Stem=True):\n",
    "\n",
    "    for col in columnas:\n",
    "        df_textos[col] = PreProcesar_w5(list(df_textos[col]), POS, Lema, Stem)\n",
    "    \n",
    "    # Sanear el DataFrame eliminando los duplicados y reindexándolo\n",
    "    df_textos = df_textos.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return df_textos\n",
    "\n",
    "def w5_listo(df_textos):\n",
    "    vari_df_textos_w5= df_textos.copy()\n",
    "    w5_corr = Corregir_Documentos_w5(vari_df_textos_w5,['oracion'],False,True,True)\n",
    "    vectorizador_w5 = pickle.load(open(\"vectorizador_w5.pkl\",\"rb\"))\n",
    "    array_w5=vectorizador_w5.transform([w5_corr['oracion'][0][:-1]])\n",
    "    modelo_w5 = pickle.load(open(\"modelo_w5.sav\",\"rb\"))\n",
    "\n",
    "    w5 = sorted(list(modelo_w5.predict_proba(array_w5)[0]))[-1]\n",
    "    w5_2 = modelo_w5.predict(array_w5)\n",
    "    if w5 < 0.7: w5_2[0] = \"todas\"\n",
    "    return w5_2,w5\n",
    "#####################################################################################################\n",
    "def PreProcesar_intents(Corpus, POS=False, Lema=True, Stem=True):\n",
    "    \n",
    "    \n",
    "    # Generar una lista de documentos de spacy para tratar el POS Tagging y la Lematización\n",
    "    docs=[]\n",
    "    for oracion in Corpus:\n",
    "        docs.append(nlp(oracion.lower())) #La lematización funciona mejor en minúsculas\n",
    "    \n",
    "    # Crear una lista de oraciones, donde cada elemento es una lista de palabras.\n",
    "    # Cada palabra está definida por una tupla (Texto, POSTag, Lema)\n",
    "    # Se omiten los tokens que son identificados como signos de puntuación\n",
    "    oraciones=[]\n",
    "    for doc in docs:\n",
    "        oracion=[]\n",
    "        for token in doc:\n",
    "            if token.pos_ != 'PUNCT':\n",
    "                oracion.append((token.text, token.pos_, token.lemma_))\n",
    "        oraciones.append(oracion)\n",
    "    \n",
    "    ww = open('stopwords_intents.txt', 'r', encoding='utf8')\n",
    "    stopwords = ww.read().split('\\n')\n",
    "    stopwords=[x.lower() for x in stopwords]\n",
    "    ww.close()\n",
    "    oraciones = [[palabra for palabra in oracion if palabra[2] not in stopwords] for oracion in oraciones]\n",
    "    \n",
    "    # Stemming\n",
    "    if Stem==True:\n",
    "        oraciones_aux=[]\n",
    "        for oracion in oraciones:\n",
    "            oracion_aux=[]\n",
    "            for palabra in oracion:\n",
    "                p_texto, p_pos, p_lema = palabra\n",
    "                # Si Lema es True, se Stemmatiza el lema; si no, se Stemmatiza la palabra original\n",
    "                if Lema==True:\n",
    "                    oracion_aux.append((p_texto, p_pos, p_lema, spanish_stemmer.stem(p_lema)))\n",
    "                else:\n",
    "                    oracion_aux.append((p_texto, p_pos, p_lema, spanish_stemmer.stem(p_texto)))\n",
    "            oraciones_aux.append(oracion_aux)\n",
    "        \n",
    "        oraciones = oraciones_aux\n",
    "    \n",
    "    Corpus_Procesado = [] #Variable de salida\n",
    "    \n",
    "    for doc in oraciones:\n",
    "        oracion = ''\n",
    "        for palabra in doc:\n",
    "            if Stem == True:\n",
    "                # Devolver cadena de Stemming\n",
    "                oracion = oracion + palabra[3]\n",
    "            else:\n",
    "                if Lema == True:\n",
    "                    # Devolver cadena de Lemas\n",
    "                    oracion = oracion + palabra[2]\n",
    "                else:\n",
    "                    # Devolver cadena de palabras originales\n",
    "                    oracion = oracion + palabra[0]\n",
    "            \n",
    "            if POS == True:\n",
    "                #Concatenar POS a cada palabra\n",
    "                oracion = oracion + '_' + palabra[1].lower()\n",
    "            \n",
    "            oracion = oracion + ' '\n",
    "        \n",
    "        Corpus_Procesado.append(oracion)\n",
    "        \n",
    "    return Corpus_Procesado\n",
    "\n",
    "def Corregir_Documentos_intents(df_textos, columnas, POS=False, Lema=True, Stem=True):\n",
    "\n",
    "    for col in columnas:\n",
    "        df_textos[col] = PreProcesar_intents(list(df_textos[col]), POS, Lema, Stem)\n",
    "    \n",
    "    # Sanear el DataFrame eliminando los duplicados y reindexándolo\n",
    "    df_textos = df_textos.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return df_textos\n",
    "\n",
    "def inte_listo(df_textos):\n",
    "    vari_df_textos_inte = df_textos.copy()\n",
    "    inte_corr = Corregir_Documentos_intents(vari_df_textos_inte,['oracion'],False,True,True)\n",
    "    vectorizador_intents = pickle.load(open(\"vectorizador_intents.pkl\",\"rb\"))\n",
    "    array_intents=vectorizador_intents.transform([inte_corr['oracion'][0][:-1]])\n",
    "\n",
    "    modelo_intents = pickle.load(open(\"modelo_intents.sav\",\"rb\"))\n",
    "    intents = sorted(list(modelo_intents.predict_proba(array_intents)[0]))[-1]\n",
    "    intents_2 = modelo_intents.predict(array_intents)\n",
    "    if intents < 0.7: intents_2[0] = \"generalidades\"\n",
    "    return intents_2,intents\n",
    "#####################################################################################################\n",
    "def sub_inte_listo(df_textos):\n",
    "    vari_df_textos_sub_inte= df_textos.copy()\n",
    "    sub_inte_corr = Corregir_Documentos_intents(vari_df_textos_sub_inte,['oracion'],False,True,True)\n",
    "    vectorizador_sub_intents = pickle.load(open(\"vectorizador_sub_intents.pkl\",\"rb\"))\n",
    "    array_sub_intents=vectorizador_sub_intents.transform([sub_inte_corr['oracion'][0][:-1]])\n",
    "\n",
    "    modelo_sub_intents = pickle.load(open(\"modelo_sub_intents.sav\",\"rb\"))\n",
    "    sub_intents = sorted(list(modelo_sub_intents.predict_proba(array_sub_intents)[0]))[-1]\n",
    "    sub_intents_2 = modelo_sub_intents.predict(array_sub_intents)\n",
    "    if sub_intents < 0.7: sub_intents_2[0] = \"todas\"\n",
    "    return sub_intents_2,sub_intents\n",
    "#####################################################################################################\n",
    "def pregunta_lista_(df_textos):\n",
    "    \n",
    "    carrera_ = carrera_lista(df_textos)\n",
    "    \n",
    "    w5_ = w5_listo(df_textos)\n",
    "    \n",
    "    inte_ = inte_listo(df_textos)\n",
    "    \n",
    "    sub_inte_ = sub_inte_listo(df_textos)\n",
    "    \n",
    "    listita = [inte_[0][0], sub_inte_[0][0], carrera_[0][0], w5_[0][0]]\n",
    "    df_textos_listo = pd.DataFrame(columns = ['Intencion',\"SubIntencion\",\"Carrera\",\"w5\"])\n",
    "    df_textos_listo.loc[0] = listita\n",
    "    return df_textos_listo \n",
    "#####################################################################################################\n",
    "def respuesta(df_textos):\n",
    "    pregunta_lista = pregunta_lista_(df_textos)\n",
    "    respu = respuestas[(respuestas.Intencion==pregunta_lista[\"Intencion\"].values[0])\\\n",
    "                & (respuestas.SubIntencion==pregunta_lista[\"SubIntencion\"].values[0])\\\n",
    "                & (respuestas.Carrera==pregunta_lista[\"Carrera\"].values[0])\\\n",
    "                & ((respuestas.w5==pregunta_lista[\"w5\"].values[0]) | (respuestas.w5==\"todas\"))] \n",
    "    if len(respu.index) < 1: print(\"Perdon, no entendi bien la pregunta. ¿Podrias reformularla? :D \")                \n",
    "    else: print(str(respu[\"Respuesta\"].values[0]))\n",
    "    #return respu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En qué te puedo ayudar?\n",
      "\n",
      "bucle_prin\n",
      "                       oracion\n",
      "0  quiero estudiar informatica\n",
      "vari_df_textos_carr                        oracion\n",
      "0  quiero estudiar informatica\n",
      "carrera_lista carrera_corr\n",
      "                 oracion\n",
      "0  quer estudi informat \n",
      "['3d', 'administr', 'administr contabl', 'administr de', 'administr turist', 'agenci', 'agenci de', 'agricol', 'agro', 'agronegoci', 'agronomi', 'agropecuari', 'al', 'al exterior', 'algo', 'algo sobr', 'ambiental', 'anal', 'anal de', 'artificial', 'artificial cienci', 'artist', 'artist grafic', 'asistent', 'asistent contabl', 'asistent de', 'automatiz', 'automatiz control', 'automatiz de', 'camp', 'cienci', 'cienci de', 'comerci', 'comerci exterior', 'comerci internacional', 'comercializ', 'comput', 'contabil', 'contabl', 'contabl financier', 'contador', 'control', 'control de', 'control robotic', 'creacion', 'creacion de', 'dat', 'de', 'de contador', 'de dat', 'de empres', 'de jueg', 'de multimedi', 'de proces', 'de public', 'de robot', 'de sistem', 'de turism', 'de videojueg', 'del', 'del camp', 'diseñ', 'diseñ 3d', 'diseñ de', 'diseñ grafic', 'diseñ impresion', 'empres', 'exterior', 'financier', 'gestion', 'gestion contabl', 'gestion de', 'gestion financier', 'gestion logist', 'gestion turist', 'grafic', 'higien', 'higien segur', 'human', 'ia', 'impresion', 'impresion 3d', 'informatic', 'institucional', 'inteligent', 'inteligent artificial', 'internacional', 'jueg', 'logist', 'marketing', 'mecatron', 'medi', 'multi', 'multi medi', 'multimedi', 'mutimedi', 'negoci', 'negoci agricol', 'negoci agropecuari', 'negoci del', 'proces', 'program', 'propagand', 'public', 'public institucional', 'recurs', 'recurs human', 'relacion', 'relacion public', 'robot', 'robotic', 'segur', 'segur ambiental', 'simul', 'simul virtual', 'sistem', 'sobr', 'sobr empres', 'turism', 'turist', 'vent', 'vent al', 'videojueg', 'virtual', 'virtual videojueg']\n",
      "Perdon, no entendi bien la pregunta. ¿Podrias reformularla? :D \n",
      "Tiene otra pregunta? s/n\n",
      "Opcion incorrecta\n",
      "En qué te puedo ayudar?\n",
      "\n",
      "bucle_prin\n",
      "  oracion\n",
      "0       c\n",
      "vari_df_textos_carr   oracion\n",
      "0       c\n",
      "carrera_lista carrera_corr\n",
      "  oracion\n",
      "0      c \n",
      "['3d', 'administr', 'administr contabl', 'administr de', 'administr turist', 'agenci', 'agenci de', 'agricol', 'agro', 'agronegoci', 'agronomi', 'agropecuari', 'al', 'al exterior', 'algo', 'algo sobr', 'ambiental', 'anal', 'anal de', 'artificial', 'artificial cienci', 'artist', 'artist grafic', 'asistent', 'asistent contabl', 'asistent de', 'automatiz', 'automatiz control', 'automatiz de', 'camp', 'cienci', 'cienci de', 'comerci', 'comerci exterior', 'comerci internacional', 'comercializ', 'comput', 'contabil', 'contabl', 'contabl financier', 'contador', 'control', 'control de', 'control robotic', 'creacion', 'creacion de', 'dat', 'de', 'de contador', 'de dat', 'de empres', 'de jueg', 'de multimedi', 'de proces', 'de public', 'de robot', 'de sistem', 'de turism', 'de videojueg', 'del', 'del camp', 'diseñ', 'diseñ 3d', 'diseñ de', 'diseñ grafic', 'diseñ impresion', 'empres', 'exterior', 'financier', 'gestion', 'gestion contabl', 'gestion de', 'gestion financier', 'gestion logist', 'gestion turist', 'grafic', 'higien', 'higien segur', 'human', 'ia', 'impresion', 'impresion 3d', 'informatic', 'institucional', 'inteligent', 'inteligent artificial', 'internacional', 'jueg', 'logist', 'marketing', 'mecatron', 'medi', 'multi', 'multi medi', 'multimedi', 'mutimedi', 'negoci', 'negoci agricol', 'negoci agropecuari', 'negoci del', 'proces', 'program', 'propagand', 'public', 'public institucional', 'recurs', 'recurs human', 'relacion', 'relacion public', 'robot', 'robotic', 'segur', 'segur ambiental', 'simul', 'simul virtual', 'sistem', 'sobr', 'sobr empres', 'turism', 'turist', 'vent', 'vent al', 'videojueg', 'virtual', 'virtual videojueg']\n",
      "Perdon, no entendi bien la pregunta. ¿Podrias reformularla? :D \n",
      "Tiene otra pregunta? s/n\n",
      "Opcion incorrecta\n",
      "En qué te puedo ayudar?\n",
      "\n",
      "bucle_prin\n",
      "  oracion\n",
      "0       c\n",
      "vari_df_textos_carr   oracion\n",
      "0       c\n",
      "carrera_lista carrera_corr\n",
      "  oracion\n",
      "0      c \n",
      "['3d', 'administr', 'administr contabl', 'administr de', 'administr turist', 'agenci', 'agenci de', 'agricol', 'agro', 'agronegoci', 'agronomi', 'agropecuari', 'al', 'al exterior', 'algo', 'algo sobr', 'ambiental', 'anal', 'anal de', 'artificial', 'artificial cienci', 'artist', 'artist grafic', 'asistent', 'asistent contabl', 'asistent de', 'automatiz', 'automatiz control', 'automatiz de', 'camp', 'cienci', 'cienci de', 'comerci', 'comerci exterior', 'comerci internacional', 'comercializ', 'comput', 'contabil', 'contabl', 'contabl financier', 'contador', 'control', 'control de', 'control robotic', 'creacion', 'creacion de', 'dat', 'de', 'de contador', 'de dat', 'de empres', 'de jueg', 'de multimedi', 'de proces', 'de public', 'de robot', 'de sistem', 'de turism', 'de videojueg', 'del', 'del camp', 'diseñ', 'diseñ 3d', 'diseñ de', 'diseñ grafic', 'diseñ impresion', 'empres', 'exterior', 'financier', 'gestion', 'gestion contabl', 'gestion de', 'gestion financier', 'gestion logist', 'gestion turist', 'grafic', 'higien', 'higien segur', 'human', 'ia', 'impresion', 'impresion 3d', 'informatic', 'institucional', 'inteligent', 'inteligent artificial', 'internacional', 'jueg', 'logist', 'marketing', 'mecatron', 'medi', 'multi', 'multi medi', 'multimedi', 'mutimedi', 'negoci', 'negoci agricol', 'negoci agropecuari', 'negoci del', 'proces', 'program', 'propagand', 'public', 'public institucional', 'recurs', 'recurs human', 'relacion', 'relacion public', 'robot', 'robotic', 'segur', 'segur ambiental', 'simul', 'simul virtual', 'sistem', 'sobr', 'sobr empres', 'turism', 'turist', 'vent', 'vent al', 'videojueg', 'virtual', 'virtual videojueg']\n",
      "Perdon, no entendi bien la pregunta. ¿Podrias reformularla? :D \n",
      "Tiene otra pregunta? s/n\n",
      "Opcion incorrecta\n",
      "En qué te puedo ayudar?\n",
      "\n",
      "bucle_prin\n",
      "  oracion\n",
      "0        \n",
      "vari_df_textos_carr   oracion\n",
      "0        \n",
      "carrera_lista carrera_corr\n",
      "  oracion\n",
      "0        \n",
      "['3d', 'administr', 'administr contabl', 'administr de', 'administr turist', 'agenci', 'agenci de', 'agricol', 'agro', 'agronegoci', 'agronomi', 'agropecuari', 'al', 'al exterior', 'algo', 'algo sobr', 'ambiental', 'anal', 'anal de', 'artificial', 'artificial cienci', 'artist', 'artist grafic', 'asistent', 'asistent contabl', 'asistent de', 'automatiz', 'automatiz control', 'automatiz de', 'camp', 'cienci', 'cienci de', 'comerci', 'comerci exterior', 'comerci internacional', 'comercializ', 'comput', 'contabil', 'contabl', 'contabl financier', 'contador', 'control', 'control de', 'control robotic', 'creacion', 'creacion de', 'dat', 'de', 'de contador', 'de dat', 'de empres', 'de jueg', 'de multimedi', 'de proces', 'de public', 'de robot', 'de sistem', 'de turism', 'de videojueg', 'del', 'del camp', 'diseñ', 'diseñ 3d', 'diseñ de', 'diseñ grafic', 'diseñ impresion', 'empres', 'exterior', 'financier', 'gestion', 'gestion contabl', 'gestion de', 'gestion financier', 'gestion logist', 'gestion turist', 'grafic', 'higien', 'higien segur', 'human', 'ia', 'impresion', 'impresion 3d', 'informatic', 'institucional', 'inteligent', 'inteligent artificial', 'internacional', 'jueg', 'logist', 'marketing', 'mecatron', 'medi', 'multi', 'multi medi', 'multimedi', 'mutimedi', 'negoci', 'negoci agricol', 'negoci agropecuari', 'negoci del', 'proces', 'program', 'propagand', 'public', 'public institucional', 'recurs', 'recurs human', 'relacion', 'relacion public', 'robot', 'robotic', 'segur', 'segur ambiental', 'simul', 'simul virtual', 'sistem', 'sobr', 'sobr empres', 'turism', 'turist', 'vent', 'vent al', 'videojueg', 'virtual', 'virtual videojueg']\n",
      "Perdon, no entendi bien la pregunta. ¿Podrias reformularla? :D \n",
      "Tiene otra pregunta? s/n\n",
      "Opcion incorrecta\n",
      "En qué te puedo ayudar?\n",
      "\n",
      "bucle_prin\n",
      "  oracion\n",
      "0        \n",
      "vari_df_textos_carr   oracion\n",
      "0        \n",
      "carrera_lista carrera_corr\n",
      "  oracion\n",
      "0        \n",
      "['3d', 'administr', 'administr contabl', 'administr de', 'administr turist', 'agenci', 'agenci de', 'agricol', 'agro', 'agronegoci', 'agronomi', 'agropecuari', 'al', 'al exterior', 'algo', 'algo sobr', 'ambiental', 'anal', 'anal de', 'artificial', 'artificial cienci', 'artist', 'artist grafic', 'asistent', 'asistent contabl', 'asistent de', 'automatiz', 'automatiz control', 'automatiz de', 'camp', 'cienci', 'cienci de', 'comerci', 'comerci exterior', 'comerci internacional', 'comercializ', 'comput', 'contabil', 'contabl', 'contabl financier', 'contador', 'control', 'control de', 'control robotic', 'creacion', 'creacion de', 'dat', 'de', 'de contador', 'de dat', 'de empres', 'de jueg', 'de multimedi', 'de proces', 'de public', 'de robot', 'de sistem', 'de turism', 'de videojueg', 'del', 'del camp', 'diseñ', 'diseñ 3d', 'diseñ de', 'diseñ grafic', 'diseñ impresion', 'empres', 'exterior', 'financier', 'gestion', 'gestion contabl', 'gestion de', 'gestion financier', 'gestion logist', 'gestion turist', 'grafic', 'higien', 'higien segur', 'human', 'ia', 'impresion', 'impresion 3d', 'informatic', 'institucional', 'inteligent', 'inteligent artificial', 'internacional', 'jueg', 'logist', 'marketing', 'mecatron', 'medi', 'multi', 'multi medi', 'multimedi', 'mutimedi', 'negoci', 'negoci agricol', 'negoci agropecuari', 'negoci del', 'proces', 'program', 'propagand', 'public', 'public institucional', 'recurs', 'recurs human', 'relacion', 'relacion public', 'robot', 'robotic', 'segur', 'segur ambiental', 'simul', 'simul virtual', 'sistem', 'sobr', 'sobr empres', 'turism', 'turist', 'vent', 'vent al', 'videojueg', 'virtual', 'virtual videojueg']\n",
      "Perdon, no entendi bien la pregunta. ¿Podrias reformularla? :D \n",
      "Tiene otra pregunta? s/n\n",
      "Opcion incorrecta\n",
      "En qué te puedo ayudar?\n",
      "\n",
      "bucle_prin"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-388-4182e079e401>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mdf_textos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'oracion'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mdf_textos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpregunta\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bucle_prin\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_textos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#borrar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mrespuesta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_textos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, string)\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m             \u001b[1;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m                 \u001b[1;31m# mp.Pool cannot be trusted to flush promptly (or ever),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;31m# wake event thread (message content is ignored)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\zmq\\sugar\\socket.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    489\u001b[0m                                  copy_threshold=self.copy_threshold)\n\u001b[0;32m    490\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "on = True\n",
    "while on:\n",
    "    print(\"En qué te puedo ayudar?\\n\")\n",
    "    pregunta = input(\"tu: \")\n",
    "    #print(\"tu: \",pregunta)\n",
    "    df_textos = pd.DataFrame(columns = ['oracion'])\n",
    "    df_textos.loc[0] = [pregunta] \n",
    "    print(\"bucle_prin\")\n",
    "    print(df_textos)#borrar\n",
    "    respuesta(df_textos)\n",
    "    print(\"Tiene otra pregunta? s/n\")\n",
    "    rta=input()\n",
    "    if rta==\"s\":\n",
    "        on=True\n",
    "    elif rta==\"n\":\n",
    "        on=False\n",
    "    elif (rta != \"s\" ) | (rta != \"n\"):\n",
    "        print(\"Opcion incorrecta\")\n",
    "#quiero estudiar informatica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d333c3e79956f6cfdda154d497169890c9e1b3b648807dd58683480f0849f8e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
